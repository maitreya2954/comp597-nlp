{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import ipdb\n",
    "import random\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from vocab import Vocabulary, build_vocab\n",
    "from accumulator import Accumulator\n",
    "from file_io import load_sent, write_sent\n",
    "from utils import *\n",
    "from nn import *\n",
    "import beam_search, greedy_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arguments():\n",
    "    train=''\n",
    "    dev=''\n",
    "    vocab=''\n",
    "    model=''\n",
    "    test=''\n",
    "    output=''\n",
    "    embedding=''\n",
    "    dim_emb=100\n",
    "    load_model=False\n",
    "    batch_size=64\n",
    "    max_epochs=20\n",
    "    steps_per_checkpoint=10\n",
    "    max_seq_length=20\n",
    "    max_train_size=1500\n",
    "    beam=1\n",
    "    dropout_keep_prob=0.5\n",
    "    n_layers=1\n",
    "    dim_y=200\n",
    "    dim_z=500\n",
    "    learning_rate=0.0005\n",
    "    rho=1\n",
    "    gamma_init=0.1\n",
    "    gamma_decay=1\n",
    "    gamma_min=0.1\n",
    "    filter_sizes=\"1,2,3,4,5\"\n",
    "    n_filters=128\n",
    "    online_testing=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def __init__(self, arguments, vocab):\n",
    "        dim_y = arguments.dim_y\n",
    "        dim_z = arguments.dim_z\n",
    "        dim_h = dim_y + dim_z\n",
    "        dim_emb = arguments.dim_emb\n",
    "        n_layers = arguments.n_layers\n",
    "        max_len = arguments.max_seq_length\n",
    "        filter_sizes = [int(x) for x in arguments.filter_sizes.split(',')]\n",
    "        n_filters = arguments.n_filters\n",
    "        beta1, beta2 = 0.5, 0.999\n",
    "        grad_clip = 30.0\n",
    "\n",
    "        self.dropout = tf.placeholder(tf.float32,\n",
    "            name='dropout')\n",
    "        self.learning_rate = tf.placeholder(tf.float32,\n",
    "            name='learning_rate')\n",
    "        self.rho = tf.placeholder(tf.float32,\n",
    "            name='rho')\n",
    "        self.gamma = tf.placeholder(tf.float32,\n",
    "            name='gamma')\n",
    "\n",
    "        self.batch_len = tf.placeholder(tf.int32,\n",
    "            name='batch_len')\n",
    "        self.batch_size = tf.placeholder(tf.int32,\n",
    "            name='batch_size')\n",
    "        self.enc_inputs = tf.placeholder(tf.int32, [None, None],    #size * len\n",
    "            name='enc_inputs')\n",
    "        self.dec_inputs = tf.placeholder(tf.int32, [None, None],\n",
    "            name='dec_inputs')\n",
    "        self.targets = tf.placeholder(tf.int32, [None, None],\n",
    "            name='targets')\n",
    "        self.weights = tf.placeholder(tf.float32, [None, None],\n",
    "            name='weights')\n",
    "        self.labels = tf.placeholder(tf.float32, [None],\n",
    "            name='labels')\n",
    "\n",
    "        labels = tf.reshape(self.labels, [-1, 1])\n",
    "\n",
    "        embedding = tf.get_variable('embedding',\n",
    "            initializer=vocab.embedding.astype(np.float32))\n",
    "        with tf.variable_scope('projection'):\n",
    "            proj_W = tf.get_variable('W', [dim_h, vocab.size])\n",
    "            proj_b = tf.get_variable('b', [vocab.size])\n",
    "\n",
    "        enc_inputs = tf.nn.embedding_lookup(embedding, self.enc_inputs)\n",
    "        dec_inputs = tf.nn.embedding_lookup(embedding, self.dec_inputs)\n",
    "\n",
    "        #####   auto-encoder   #####\n",
    "        init_state = tf.concat([linear(labels, dim_y, scope='encoder'),\n",
    "            tf.zeros([self.batch_size, dim_z])], 1)\n",
    "        cell_e = create_cell(dim_h, n_layers, self.dropout)\n",
    "        _, z = tf.nn.dynamic_rnn(cell_e, enc_inputs,\n",
    "            initial_state=init_state, scope='encoder')\n",
    "        z = z[:, dim_y:]\n",
    "\n",
    "        #cell_e = create_cell(dim_z, n_layers, self.dropout)\n",
    "        #_, z = tf.nn.dynamic_rnn(cell_e, enc_inputs,\n",
    "        #    dtype=tf.float32, scope='encoder')\n",
    "\n",
    "        self.h_ori = tf.concat([linear(labels, dim_y,\n",
    "            scope='generator'), z], 1)\n",
    "        self.h_tsf = tf.concat([linear(1-labels, dim_y,\n",
    "            scope='generator', reuse=True), z], 1)\n",
    "\n",
    "        cell_g = create_cell(dim_h, n_layers, self.dropout)\n",
    "        g_outputs, _ = tf.nn.dynamic_rnn(cell_g, dec_inputs,\n",
    "            initial_state=self.h_ori, scope='generator')\n",
    "\n",
    "        # attach h0 in the front\n",
    "        teach_h = tf.concat([tf.expand_dims(self.h_ori, 1), g_outputs], 1)\n",
    "\n",
    "        g_outputs = tf.nn.dropout(g_outputs, self.dropout)\n",
    "        g_outputs = tf.reshape(g_outputs, [-1, dim_h])\n",
    "        g_logits = tf.matmul(g_outputs, proj_W) + proj_b\n",
    "\n",
    "        loss_rec = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=tf.reshape(self.targets, [-1]), logits=g_logits)\n",
    "        loss_rec *= tf.reshape(self.weights, [-1])\n",
    "        self.loss_rec = tf.reduce_sum(loss_rec) / tf.to_float(self.batch_size)\n",
    "\n",
    "        #####   feed-previous decoding   #####\n",
    "        go = dec_inputs[:,0,:]\n",
    "        soft_func = softsample_word(self.dropout, proj_W, proj_b, embedding,\n",
    "            self.gamma)\n",
    "        hard_func = argmax_word(self.dropout, proj_W, proj_b, embedding)\n",
    "\n",
    "        soft_h_ori, soft_logits_ori = rnn_decode(self.h_ori, go, max_len,\n",
    "            cell_g, soft_func, scope='generator')\n",
    "        soft_h_tsf, soft_logits_tsf = rnn_decode(self.h_tsf, go, max_len,\n",
    "            cell_g, soft_func, scope='generator')\n",
    "\n",
    "        hard_h_ori, self.hard_logits_ori = rnn_decode(self.h_ori, go, max_len,\n",
    "            cell_g, hard_func, scope='generator')\n",
    "        hard_h_tsf, self.hard_logits_tsf = rnn_decode(self.h_tsf, go, max_len,\n",
    "            cell_g, hard_func, scope='generator')\n",
    "\n",
    "        #####   discriminator   #####\n",
    "        # a batch's first half consists of sentences of one style,\n",
    "        # and second half of the other\n",
    "        half = self.batch_size / 2\n",
    "        zeros, ones = self.labels[:half], self.labels[half:]\n",
    "        soft_h_tsf = soft_h_tsf[:, :1+self.batch_len, :]\n",
    "\n",
    "        self.loss_d0, loss_g0 = discriminator(teach_h[:half], soft_h_tsf[half:],\n",
    "            ones, zeros, filter_sizes, n_filters, self.dropout,\n",
    "            scope='discriminator0')\n",
    "        self.loss_d1, loss_g1 = discriminator(teach_h[half:], soft_h_tsf[:half],\n",
    "            ones, zeros, filter_sizes, n_filters, self.dropout,\n",
    "            scope='discriminator1')\n",
    "\n",
    "        #####   optimizer   #####\n",
    "        self.loss_adv = loss_g0 + loss_g1\n",
    "        self.loss = self.loss_rec + self.rho * self.loss_adv\n",
    "\n",
    "        theta_eg = retrive_var(['encoder', 'generator',\n",
    "            'embedding', 'projection'])\n",
    "        theta_d0 = retrive_var(['discriminator0'])\n",
    "        theta_d1 = retrive_var(['discriminator1'])\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(self.learning_rate, beta1, beta2)\n",
    "\n",
    "        grad_rec, _ = zip(*opt.compute_gradients(self.loss_rec, theta_eg))\n",
    "        grad_adv, _ = zip(*opt.compute_gradients(self.loss_adv, theta_eg))\n",
    "        grad, _ = zip(*opt.compute_gradients(self.loss, theta_eg))\n",
    "        grad, _ = tf.clip_by_global_norm(grad, grad_clip)\n",
    "\n",
    "        self.grad_rec_norm = tf.global_norm(grad_rec)\n",
    "        self.grad_adv_norm = tf.global_norm(grad_adv)\n",
    "        self.grad_norm = tf.global_norm(grad)\n",
    "\n",
    "        self.optimize_tot = opt.apply_gradients(zip(grad, theta_eg))\n",
    "        self.optimize_rec = opt.minimize(self.loss_rec, var_list=theta_eg)\n",
    "        self.optimize_d0 = opt.minimize(self.loss_d0, var_list=theta_d0)\n",
    "        self.optimize_d1 = opt.minimize(self.loss_d1, var_list=theta_d1)\n",
    "\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sess, arguments, vocab):\n",
    "    model = Model(arguments, vocab)\n",
    "    if arguments.load_model:\n",
    "        print 'Loading model from', arguments.model\n",
    "        model.saver.restore(sess, arguments.model)\n",
    "    else:\n",
    "        print 'Creating model with fresh parameters.'\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer(model, decoder, sess, arguments, vocab, data0, data1, out_path):\n",
    "    batches, order0, order1 = get_batches(data0, data1,\n",
    "        vocab.word2id, arguments.batch_size)\n",
    "    print 'Output path: ', out_path\n",
    "    #data0_rec, data1_rec = [], []\n",
    "    data0_tsf, data1_tsf = [], []\n",
    "    losses = Accumulator(len(batches), ['loss', 'rec', 'adv', 'd0', 'd1'])\n",
    "    print 'Transfer in batches', len(batches)\n",
    "    for batch in batches:\n",
    "        rec, tsf = decoder.rewrite(batch)\n",
    "        half = batch['size'] / 2\n",
    "        #data0_rec += rec[:half]\n",
    "        #data1_rec += rec[half:]\n",
    "        data0_tsf += tsf[:half]\n",
    "        data1_tsf += tsf[half:]\n",
    "\n",
    "        loss, loss_rec, loss_adv, loss_d0, loss_d1 = sess.run([model.loss,\n",
    "            model.loss_rec, model.loss_adv, model.loss_d0, model.loss_d1],\n",
    "            feed_dict=feed_dictionary(model, batch, arguments.rho, arguments.gamma_min))\n",
    "        losses.add([loss, loss_rec, loss_adv, loss_d0, loss_d1])\n",
    "    \n",
    "    n0, n1 = len(data0), len(data1)\n",
    "    #data0_rec = reorder(order0, data0_rec)[:n0]\n",
    "    #data1_rec = reorder(order1, data1_rec)[:n1]\n",
    "    data0_tsf = reorder(order0, data0_tsf)[:n0]\n",
    "    data1_tsf = reorder(order1, data1_tsf)[:n1]\n",
    "\n",
    "    if out_path:\n",
    "        #write_sent(data0_rec, out_path+'.0'+'.rec')\n",
    "        #write_sent(data1_rec, out_path+'.1'+'.rec')\n",
    "        write_sent(data0_tsf, out_path+'.0'+'.tsf')\n",
    "        write_sent(data1_tsf, out_path+'.1'+'.tsf')\n",
    "    print 'Write completed'\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments.train=\"../data/yelp/sentiment.train\"\n",
    "arguments.dev=\"../data/yelp/sentiment.dev\"\n",
    "arguments.vocab=\"../tmp/yelp.vocab\"\n",
    "arguments.model=\"../tmp/model\"\n",
    "arguments.output=\"../tmp/sentiment.dev\"\n",
    "\n",
    "# arguments.test=\"../data/yelp/sentiment.test\"\n",
    "# arguments.output=\"../tmp/sentiment.test\"\n",
    "# arguments.load_model=True\n",
    "# arguments.beam=8\n",
    "# arguments.train=''\n",
    "# arguments.dev=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sents of training file 0: 1500\n",
      "#sents of training file 1: 1500\n"
     ]
    }
   ],
   "source": [
    "if arguments.train:\n",
    "    train0 = load_sent(arguments.train + '.0', arguments.max_train_size)\n",
    "    train1 = load_sent(arguments.train + '.1', arguments.max_train_size)\n",
    "\n",
    "    print '#sents of training file 0:', len(train0)\n",
    "    print '#sents of training file 1:', len(train1)\n",
    "\n",
    "    if not os.path.isfile(arguments.vocab):\n",
    "        build_vocab(train0 + train1, arguments.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 9357\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary(arguments.vocab, arguments.embedding, arguments.dim_emb)\n",
    "print 'vocabulary size:', vocab.size\n",
    "\n",
    "if arguments.dev:\n",
    "    dev0 = load_sent(arguments.dev + '.0', arguments.max_train_size)\n",
    "    dev1 = load_sent(arguments.dev + '.1', arguments.max_train_size)\n",
    "\n",
    "if arguments.test:\n",
    "    test0 = load_sent(arguments.test + '.0', arguments.max_train_size)\n",
    "    test1 = load_sent(arguments.test + '.1', arguments.max_train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(): \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = create_model(sess, arguments, vocab)\n",
    "\n",
    "        if arguments.beam > 1:\n",
    "            decoder = beam_search.Decoder(sess, arguments, vocab, model)\n",
    "        else:\n",
    "            decoder = greedy_decoding.Decoder(sess, arguments, vocab, model)\n",
    "\n",
    "        if arguments.train:\n",
    "            batches, _, _ = get_batches(train0, train1, vocab.word2id,\n",
    "                arguments.batch_size, noisy=True)\n",
    "            random.shuffle(batches)\n",
    "\n",
    "            start_time = time.time()\n",
    "            step = 0\n",
    "            losses = Accumulator(arguments.steps_per_checkpoint,\n",
    "                ['loss', 'rec', 'adv', 'd0', 'd1'])\n",
    "            best_dev = float('inf')\n",
    "            learning_rate = arguments.learning_rate\n",
    "            rho = arguments.rho\n",
    "            gamma = arguments.gamma_init\n",
    "            dropout = arguments.dropout_keep_prob\n",
    "\n",
    "            #gradients = Accumulator(arguments.steps_per_checkpoint,\n",
    "            #    ['|grad_rec|', '|grad_adv|', '|grad|'])\n",
    "\n",
    "            for epoch in range(1, 1+arguments.max_epochs):\n",
    "                print '--------------------epoch %d--------------------' % epoch\n",
    "                print 'learning_rate:', learning_rate, '  gamma:', gamma, '  batches:', len(batches)\n",
    "\n",
    "                for batch in batches:\n",
    "#                     print 'Staring batch', len(batches)\n",
    "                    feed_dict = feed_dictionary(model, batch, rho, gamma,\n",
    "                        dropout, learning_rate)\n",
    "\n",
    "                    loss_d0, _ = sess.run([model.loss_d0, model.optimize_d0],\n",
    "                        feed_dict=feed_dict)\n",
    "                    loss_d1, _ = sess.run([model.loss_d1, model.optimize_d1],\n",
    "                        feed_dict=feed_dict)\n",
    "#                     print 'Session run completed'\n",
    "                    # do not back-propagate from the discriminator\n",
    "                    # when it is too poor\n",
    "                    if loss_d0 < 1.2 and loss_d1 < 1.2:\n",
    "                        optimize = model.optimize_tot\n",
    "                    else:\n",
    "                        optimize = model.optimize_rec\n",
    "#                     print 'Model optimized'\n",
    "                    loss, loss_rec, loss_adv, _ = sess.run([model.loss,\n",
    "                        model.loss_rec, model.loss_adv, optimize],\n",
    "                        feed_dict=feed_dict)\n",
    "                    losses.add([loss, loss_rec, loss_adv, loss_d0, loss_d1])\n",
    "#                     print('Losses calculated')\n",
    "                    #grad_rec, grad_adv, grad = sess.run([model.grad_rec_norm,\n",
    "                    #    model.grad_adv_norm, model.grad_norm],\n",
    "                    #    feed_dict=feed_dict)\n",
    "                    #gradients.add([grad_rec, grad_adv, grad])\n",
    "\n",
    "                    step += 1\n",
    "                    if step % arguments.steps_per_checkpoint == 0:\n",
    "                        losses.output('step %d, time %.0fs,'\n",
    "                            % (step, time.time() - start_time))\n",
    "                        losses.clear()\n",
    "\n",
    "                        #gradients.output()\n",
    "                        #gradients.clear()\n",
    "\n",
    "                if arguments.dev:\n",
    "                    dev_losses = transfer(model, decoder, sess, arguments, vocab,\n",
    "                        dev0, dev1, arguments.output + '.epoch%d' % epoch)\n",
    "                    dev_losses.output('dev')\n",
    "                    if dev_losses.values[0] < best_dev:\n",
    "                        best_dev = dev_losses.values[0]\n",
    "                        print 'saving model...'\n",
    "                        model.saver.save(sess, arguments.model)\n",
    "\n",
    "                gamma = max(arguments.gamma_min, gamma * arguments.gamma_decay)\n",
    "\n",
    "\n",
    "        if arguments.test:\n",
    "            test_losses = transfer(model, decoder, sess, arguments, vocab,\n",
    "                test0, test1, arguments.output)\n",
    "            test_losses.output('test')\n",
    "\n",
    "        if arguments.online_testing:\n",
    "            while True:\n",
    "                sys.stdout.write('> ')\n",
    "                sys.stdout.flush()\n",
    "                inp = sys.stdin.readline().rstrip()\n",
    "                if inp == 'quit' or inp == 'exit':\n",
    "                    break\n",
    "                inp = inp.split()\n",
    "                y = int(inp[0])\n",
    "                sent = inp[1:]\n",
    "\n",
    "                batch = get_batch([sent], [y], vocab.word2id)\n",
    "                ori, tsf = decoder.rewrite(batch)\n",
    "                print 'original:', ' '.join(w for w in ori[0])\n",
    "                print 'transfer:', ' '.join(w for w in tsf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model with fresh parameters.\n",
      "--------------------epoch 1--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 10, time 16s, loss 78.29, rec 73.71, adv 4.57, d0 4.61, d1 3.54\n",
      "step 20, time 31s, loss 63.94, rec 62.42, adv 1.52, d0 1.39, d1 1.68\n",
      "Output path:  ../tmp/sentiment.dev.epoch1\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 60.14, rec 58.79, adv 1.35, d0 1.38, d1 1.41\n",
      "saving model...\n",
      "--------------------epoch 2--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 30, time 73s, loss 58.19, rec 56.77, adv 1.42, d0 1.41, d1 1.42\n",
      "step 40, time 88s, loss 65.89, rec 64.52, adv 1.38, d0 1.41, d1 1.42\n",
      "Output path:  ../tmp/sentiment.dev.epoch2\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 59.31, rec 57.90, adv 1.40, d0 1.38, d1 1.38\n",
      "saving model...\n",
      "--------------------epoch 3--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 50, time 129s, loss 49.71, rec 48.32, adv 1.39, d0 1.40, d1 1.41\n",
      "step 60, time 145s, loss 59.09, rec 57.62, adv 1.47, d0 1.40, d1 1.40\n",
      "step 70, time 161s, loss 63.16, rec 61.80, adv 1.36, d0 1.40, d1 1.39\n",
      "Output path:  ../tmp/sentiment.dev.epoch3\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 58.08, rec 56.30, adv 1.78, d0 1.49, d1 1.34\n",
      "saving model...\n",
      "--------------------epoch 4--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 80, time 202s, loss 50.32, rec 48.80, adv 1.52, d0 1.38, d1 1.39\n",
      "step 90, time 219s, loss 61.28, rec 59.66, adv 1.62, d0 1.33, d1 1.33\n",
      "Output path:  ../tmp/sentiment.dev.epoch4\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 56.98, rec 55.48, adv 1.50, d0 1.03, d1 1.08\n",
      "saving model...\n",
      "--------------------epoch 5--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 100, time 261s, loss 46.59, rec 44.73, adv 1.86, d0 1.30, d1 1.21\n",
      "step 110, time 278s, loss 58.10, rec 55.90, adv 2.20, d0 1.05, d1 1.39\n",
      "step 120, time 296s, loss 55.26, rec 53.16, adv 2.10, d0 1.07, d1 1.30\n",
      "Output path:  ../tmp/sentiment.dev.epoch5\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 56.25, rec 54.54, adv 1.72, d0 0.96, d1 1.05\n",
      "saving model...\n",
      "--------------------epoch 6--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 130, time 339s, loss 50.95, rec 48.47, adv 2.48, d0 0.96, d1 1.36\n",
      "step 140, time 357s, loss 55.77, rec 53.27, adv 2.50, d0 0.88, d1 1.39\n",
      "Output path:  ../tmp/sentiment.dev.epoch6\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 55.22, rec 53.63, adv 1.60, d0 1.01, d1 1.29\n",
      "saving model...\n",
      "--------------------epoch 7--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 150, time 401s, loss 50.60, rec 48.32, adv 2.28, d0 1.10, d1 1.32\n",
      "step 160, time 418s, loss 59.08, rec 56.53, adv 2.55, d0 0.96, d1 1.31\n",
      "Output path:  ../tmp/sentiment.dev.epoch7\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 54.74, rec 52.91, adv 1.82, d0 1.31, d1 1.23\n",
      "saving model...\n",
      "--------------------epoch 8--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 170, time 461s, loss 43.03, rec 40.59, adv 2.44, d0 1.16, d1 1.25\n",
      "step 180, time 479s, loss 52.92, rec 50.45, adv 2.47, d0 0.96, d1 1.39\n",
      "step 190, time 498s, loss 56.57, rec 53.82, adv 2.75, d0 0.97, d1 1.18\n",
      "Output path:  ../tmp/sentiment.dev.epoch8\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 53.78, rec 52.29, adv 1.49, d0 1.30, d1 1.14\n",
      "saving model...\n",
      "--------------------epoch 9--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 200, time 541s, loss 45.33, rec 43.06, adv 2.27, d0 1.23, d1 1.18\n",
      "step 210, time 559s, loss 55.56, rec 52.95, adv 2.61, d0 1.01, d1 1.36\n",
      "Output path:  ../tmp/sentiment.dev.epoch9\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 53.85, rec 52.01, adv 1.83, d0 1.51, d1 1.12\n",
      "--------------------epoch 10--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 220, time 599s, loss 41.69, rec 39.25, adv 2.44, d0 1.26, d1 1.18\n",
      "step 230, time 616s, loss 52.89, rec 50.12, adv 2.77, d0 1.15, d1 1.39\n",
      "step 240, time 634s, loss 49.61, rec 46.93, adv 2.68, d0 1.24, d1 1.25\n",
      "Output path:  ../tmp/sentiment.dev.epoch10\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 52.91, rec 51.56, adv 1.36, d0 1.64, d1 1.07\n",
      "saving model...\n",
      "--------------------epoch 11--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 250, time 678s, loss 47.64, rec 44.87, adv 2.78, d0 1.08, d1 1.12\n",
      "step 260, time 696s, loss 50.52, rec 47.86, adv 2.66, d0 1.06, d1 1.09\n",
      "Output path:  ../tmp/sentiment.dev.epoch11\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 54.68, rec 51.36, adv 3.32, d0 0.98, d1 1.48\n",
      "--------------------epoch 12--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 270, time 738s, loss 46.95, rec 44.21, adv 2.74, d0 1.18, d1 1.22\n",
      "step 280, time 756s, loss 54.28, rec 51.48, adv 2.80, d0 0.94, d1 1.24\n",
      "Output path:  ../tmp/sentiment.dev.epoch12\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 52.36, rec 50.75, adv 1.61, d0 1.67, d1 1.09\n",
      "saving model...\n",
      "--------------------epoch 13--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 290, time 800s, loss 39.32, rec 36.85, adv 2.47, d0 1.27, d1 1.09\n",
      "step 300, time 819s, loss 48.94, rec 46.24, adv 2.70, d0 1.14, d1 1.25\n",
      "step 310, time 837s, loss 52.55, rec 49.77, adv 2.78, d0 0.99, d1 1.24\n",
      "Output path:  ../tmp/sentiment.dev.epoch13\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 52.69, rec 50.53, adv 2.16, d0 1.07, d1 1.16\n",
      "--------------------epoch 14--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 320, time 879s, loss 42.37, rec 39.84, adv 2.53, d0 1.22, d1 1.11\n",
      "step 330, time 896s, loss 52.32, rec 49.44, adv 2.88, d0 0.98, d1 1.21\n",
      "Output path:  ../tmp/sentiment.dev.epoch14\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 53.41, rec 51.25, adv 2.16, d0 1.10, d1 1.06\n",
      "--------------------epoch 15--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 340, time 937s, loss 38.96, rec 36.33, adv 2.63, d0 1.34, d1 1.13\n",
      "step 350, time 956s, loss 50.57, rec 47.57, adv 3.00, d0 0.92, d1 1.28\n",
      "step 360, time 974s, loss 46.42, rec 43.45, adv 2.96, d0 1.25, d1 1.22\n",
      "Output path:  ../tmp/sentiment.dev.epoch15\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 52.58, rec 50.71, adv 1.88, d0 1.06, d1 1.14\n",
      "--------------------epoch 16--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 370, time 1015s, loss 44.27, rec 41.68, adv 2.59, d0 1.07, d1 1.28\n",
      "step 380, time 1033s, loss 46.74, rec 44.21, adv 2.53, d0 1.03, d1 1.20\n",
      "Output path:  ../tmp/sentiment.dev.epoch16\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 52.36, rec 50.43, adv 1.93, d0 1.27, d1 1.21\n",
      "--------------------epoch 17--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 390, time 1075s, loss 44.20, rec 41.48, adv 2.72, d0 1.17, d1 1.16\n",
      "step 400, time 1093s, loss 50.97, rec 48.18, adv 2.79, d0 1.07, d1 1.26\n",
      "Output path:  ../tmp/sentiment.dev.epoch17\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 53.27, rec 50.92, adv 2.35, d0 0.90, d1 1.09\n",
      "--------------------epoch 18--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 410, time 1135s, loss 37.70, rec 35.18, adv 2.52, d0 1.09, d1 1.13\n",
      "step 420, time 1152s, loss 46.46, rec 43.78, adv 2.67, d0 1.04, d1 1.37\n",
      "step 430, time 1170s, loss 48.79, rec 46.24, adv 2.55, d0 0.99, d1 1.21\n",
      "Output path:  ../tmp/sentiment.dev.epoch18\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 51.34, rec 49.60, adv 1.74, d0 1.22, d1 1.30\n",
      "saving model...\n",
      "--------------------epoch 19--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 440, time 1213s, loss 39.55, rec 37.18, adv 2.37, d0 1.33, d1 1.27\n",
      "step 450, time 1231s, loss 50.64, rec 47.43, adv 3.21, d0 1.03, d1 1.18\n",
      "Output path:  ../tmp/sentiment.dev.epoch19\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "dev loss 54.49, rec 51.33, adv 3.15, d0 0.98, d1 1.12\n",
      "--------------------epoch 20--------------------\n",
      "learning_rate: 0.0005   gamma: 0.1   batches: 24\n",
      "step 460, time 1274s, loss 37.93, rec 35.33, adv 2.60, d0 1.04, d1 1.19\n",
      "step 470, time 1294s, loss 49.20, rec 45.67, adv 3.53, d0 0.90, d1 1.12\n",
      "step 480, time 1313s, loss 45.48, rec 42.34, adv 3.14, d0 1.24, d1 1.17\n",
      "Output path:  ../tmp/sentiment.dev.epoch20\n",
      "Transfer in batches 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write completed\n",
      "dev loss 52.97, rec 50.89, adv 2.08, d0 0.80, d1 1.12\n"
     ]
    }
   ],
   "source": [
    "run_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
