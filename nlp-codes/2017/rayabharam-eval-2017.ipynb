{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import ipdb\n",
    "import random\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from vocab import Vocabulary, build_vocab\n",
    "from accumulator import Accumulator\n",
    "from file_io import load_sent, write_sent\n",
    "from utils import *\n",
    "from nn import *\n",
    "import beam_search, greedy_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arguments():\n",
    "    train=''\n",
    "    dev=''\n",
    "    vocab=''\n",
    "    model=''\n",
    "    test=''\n",
    "    output=''\n",
    "    embedding=''\n",
    "    dim_emb=100\n",
    "    load_model=False\n",
    "    batch_size=64\n",
    "    max_epochs=20\n",
    "    steps_per_checkpoint=10\n",
    "    max_seq_length=20\n",
    "    max_train_size=1500\n",
    "    beam=1\n",
    "    dropout_keep_prob=0.5\n",
    "    n_layers=1\n",
    "    dim_y=200\n",
    "    dim_z=500\n",
    "    learning_rate=0.0005\n",
    "    rho=1\n",
    "    gamma_init=0.1\n",
    "    gamma_decay=1\n",
    "    gamma_min=0.1\n",
    "    filter_sizes=\"1,2,3,4,5\"\n",
    "    n_filters=128\n",
    "    online_testing=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def __init__(self, arguments, vocab):\n",
    "        dim_y = arguments.dim_y\n",
    "        dim_z = arguments.dim_z\n",
    "        dim_h = dim_y + dim_z\n",
    "        dim_emb = arguments.dim_emb\n",
    "        n_layers = arguments.n_layers\n",
    "        max_len = arguments.max_seq_length\n",
    "        filter_sizes = [int(x) for x in arguments.filter_sizes.split(',')]\n",
    "        n_filters = arguments.n_filters\n",
    "        beta1, beta2 = 0.5, 0.999\n",
    "        grad_clip = 30.0\n",
    "\n",
    "        self.dropout = tf.placeholder(tf.float32,\n",
    "            name='dropout')\n",
    "        self.learning_rate = tf.placeholder(tf.float32,\n",
    "            name='learning_rate')\n",
    "        self.rho = tf.placeholder(tf.float32,\n",
    "            name='rho')\n",
    "        self.gamma = tf.placeholder(tf.float32,\n",
    "            name='gamma')\n",
    "\n",
    "        self.batch_len = tf.placeholder(tf.int32,\n",
    "            name='batch_len')\n",
    "        self.batch_size = tf.placeholder(tf.int32,\n",
    "            name='batch_size')\n",
    "        self.enc_inputs = tf.placeholder(tf.int32, [None, None],    #size * len\n",
    "            name='enc_inputs')\n",
    "        self.dec_inputs = tf.placeholder(tf.int32, [None, None],\n",
    "            name='dec_inputs')\n",
    "        self.targets = tf.placeholder(tf.int32, [None, None],\n",
    "            name='targets')\n",
    "        self.weights = tf.placeholder(tf.float32, [None, None],\n",
    "            name='weights')\n",
    "        self.labels = tf.placeholder(tf.float32, [None],\n",
    "            name='labels')\n",
    "\n",
    "        labels = tf.reshape(self.labels, [-1, 1])\n",
    "\n",
    "        embedding = tf.get_variable('embedding',\n",
    "            initializer=vocab.embedding.astype(np.float32))\n",
    "        with tf.variable_scope('projection'):\n",
    "            proj_W = tf.get_variable('W', [dim_h, vocab.size])\n",
    "            proj_b = tf.get_variable('b', [vocab.size])\n",
    "\n",
    "        enc_inputs = tf.nn.embedding_lookup(embedding, self.enc_inputs)\n",
    "        dec_inputs = tf.nn.embedding_lookup(embedding, self.dec_inputs)\n",
    "\n",
    "        #####   auto-encoder   #####\n",
    "        init_state = tf.concat([linear(labels, dim_y, scope='encoder'),\n",
    "            tf.zeros([self.batch_size, dim_z])], 1)\n",
    "        cell_e = create_cell(dim_h, n_layers, self.dropout)\n",
    "        _, z = tf.nn.dynamic_rnn(cell_e, enc_inputs,\n",
    "            initial_state=init_state, scope='encoder')\n",
    "        z = z[:, dim_y:]\n",
    "\n",
    "        #cell_e = create_cell(dim_z, n_layers, self.dropout)\n",
    "        #_, z = tf.nn.dynamic_rnn(cell_e, enc_inputs,\n",
    "        #    dtype=tf.float32, scope='encoder')\n",
    "\n",
    "        self.h_ori = tf.concat([linear(labels, dim_y,\n",
    "            scope='generator'), z], 1)\n",
    "        self.h_tsf = tf.concat([linear(1-labels, dim_y,\n",
    "            scope='generator', reuse=True), z], 1)\n",
    "\n",
    "        cell_g = create_cell(dim_h, n_layers, self.dropout)\n",
    "        g_outputs, _ = tf.nn.dynamic_rnn(cell_g, dec_inputs,\n",
    "            initial_state=self.h_ori, scope='generator')\n",
    "\n",
    "        # attach h0 in the front\n",
    "        teach_h = tf.concat([tf.expand_dims(self.h_ori, 1), g_outputs], 1)\n",
    "\n",
    "        g_outputs = tf.nn.dropout(g_outputs, self.dropout)\n",
    "        g_outputs = tf.reshape(g_outputs, [-1, dim_h])\n",
    "        g_logits = tf.matmul(g_outputs, proj_W) + proj_b\n",
    "\n",
    "        loss_rec = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=tf.reshape(self.targets, [-1]), logits=g_logits)\n",
    "        loss_rec *= tf.reshape(self.weights, [-1])\n",
    "        self.loss_rec = tf.reduce_sum(loss_rec) / tf.to_float(self.batch_size)\n",
    "\n",
    "        #####   feed-previous decoding   #####\n",
    "        go = dec_inputs[:,0,:]\n",
    "        soft_func = softsample_word(self.dropout, proj_W, proj_b, embedding,\n",
    "            self.gamma)\n",
    "        hard_func = argmax_word(self.dropout, proj_W, proj_b, embedding)\n",
    "\n",
    "        soft_h_ori, soft_logits_ori = rnn_decode(self.h_ori, go, max_len,\n",
    "            cell_g, soft_func, scope='generator')\n",
    "        soft_h_tsf, soft_logits_tsf = rnn_decode(self.h_tsf, go, max_len,\n",
    "            cell_g, soft_func, scope='generator')\n",
    "\n",
    "        hard_h_ori, self.hard_logits_ori = rnn_decode(self.h_ori, go, max_len,\n",
    "            cell_g, hard_func, scope='generator')\n",
    "        hard_h_tsf, self.hard_logits_tsf = rnn_decode(self.h_tsf, go, max_len,\n",
    "            cell_g, hard_func, scope='generator')\n",
    "\n",
    "        #####   discriminator   #####\n",
    "        # a batch's first half consists of sentences of one style,\n",
    "        # and second half of the other\n",
    "        half = self.batch_size / 2\n",
    "        zeros, ones = self.labels[:half], self.labels[half:]\n",
    "        soft_h_tsf = soft_h_tsf[:, :1+self.batch_len, :]\n",
    "\n",
    "        self.loss_d0, loss_g0 = discriminator(teach_h[:half], soft_h_tsf[half:],\n",
    "            ones, zeros, filter_sizes, n_filters, self.dropout,\n",
    "            scope='discriminator0')\n",
    "        self.loss_d1, loss_g1 = discriminator(teach_h[half:], soft_h_tsf[:half],\n",
    "            ones, zeros, filter_sizes, n_filters, self.dropout,\n",
    "            scope='discriminator1')\n",
    "\n",
    "        #####   optimizer   #####\n",
    "        self.loss_adv = loss_g0 + loss_g1\n",
    "        self.loss = self.loss_rec + self.rho * self.loss_adv\n",
    "\n",
    "        theta_eg = retrive_var(['encoder', 'generator',\n",
    "            'embedding', 'projection'])\n",
    "        theta_d0 = retrive_var(['discriminator0'])\n",
    "        theta_d1 = retrive_var(['discriminator1'])\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(self.learning_rate, beta1, beta2)\n",
    "\n",
    "        grad_rec, _ = zip(*opt.compute_gradients(self.loss_rec, theta_eg))\n",
    "        grad_adv, _ = zip(*opt.compute_gradients(self.loss_adv, theta_eg))\n",
    "        grad, _ = zip(*opt.compute_gradients(self.loss, theta_eg))\n",
    "        grad, _ = tf.clip_by_global_norm(grad, grad_clip)\n",
    "\n",
    "        self.grad_rec_norm = tf.global_norm(grad_rec)\n",
    "        self.grad_adv_norm = tf.global_norm(grad_adv)\n",
    "        self.grad_norm = tf.global_norm(grad)\n",
    "\n",
    "        self.optimize_tot = opt.apply_gradients(zip(grad, theta_eg))\n",
    "        self.optimize_rec = opt.minimize(self.loss_rec, var_list=theta_eg)\n",
    "        self.optimize_d0 = opt.minimize(self.loss_d0, var_list=theta_d0)\n",
    "        self.optimize_d1 = opt.minimize(self.loss_d1, var_list=theta_d1)\n",
    "\n",
    "        self.saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sess, arguments, vocab):\n",
    "    model = Model(arguments, vocab)\n",
    "    if arguments.load_model:\n",
    "        print 'Loading model from', arguments.model\n",
    "        model.saver.restore(sess, arguments.model)\n",
    "    else:\n",
    "        print 'Creating model with fresh parameters.'\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer(model, decoder, sess, arguments, vocab, data0, data1, out_path):\n",
    "    batches, order0, order1 = get_batches(data0, data1,\n",
    "        vocab.word2id, arguments.batch_size)\n",
    "    print 'Output path: ', out_path\n",
    "    #data0_rec, data1_rec = [], []\n",
    "    data0_tsf, data1_tsf = [], []\n",
    "    losses = Accumulator(len(batches), ['loss', 'rec', 'adv', 'd0', 'd1'])\n",
    "    print 'Transfer in batches', len(batches)\n",
    "    for batch in batches:\n",
    "        rec, tsf = decoder.rewrite(batch)\n",
    "        half = batch['size'] / 2\n",
    "        #data0_rec += rec[:half]\n",
    "        #data1_rec += rec[half:]\n",
    "        data0_tsf += tsf[:half]\n",
    "        data1_tsf += tsf[half:]\n",
    "\n",
    "        loss, loss_rec, loss_adv, loss_d0, loss_d1 = sess.run([model.loss,\n",
    "            model.loss_rec, model.loss_adv, model.loss_d0, model.loss_d1],\n",
    "            feed_dict=feed_dictionary(model, batch, arguments.rho, arguments.gamma_min))\n",
    "        losses.add([loss, loss_rec, loss_adv, loss_d0, loss_d1])\n",
    "    \n",
    "    n0, n1 = len(data0), len(data1)\n",
    "    #data0_rec = reorder(order0, data0_rec)[:n0]\n",
    "    #data1_rec = reorder(order1, data1_rec)[:n1]\n",
    "    data0_tsf = reorder(order0, data0_tsf)[:n0]\n",
    "    data1_tsf = reorder(order1, data1_tsf)[:n1]\n",
    "\n",
    "    if out_path:\n",
    "        #write_sent(data0_rec, out_path+'.0'+'.rec')\n",
    "        #write_sent(data1_rec, out_path+'.1'+'.rec')\n",
    "        write_sent(data0_tsf, out_path+'.0'+'.tsf')\n",
    "        write_sent(data1_tsf, out_path+'.1'+'.tsf')\n",
    "    print 'Write completed'\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments.train=\"../data/yelp/sentiment.train\"\n",
    "# arguments.dev=\"../data/yelp/sentiment.dev\"\n",
    "# arguments.vocab=\"../tmp/yelp.vocab\"\n",
    "# arguments.model=\"../tmp/model\"\n",
    "# arguments.output=\"../tmp/sentiment.dev\"\n",
    "\n",
    "\n",
    "arguments.model=\"../tmp/model\"\n",
    "arguments.test=\"../data/yelp/sentiment.test\"\n",
    "arguments.output=\"../tmp/sentiment.test\"\n",
    "arguments.vocab=\"../tmp/yelp.vocab\"\n",
    "arguments.load_model=True\n",
    "arguments.beam=8\n",
    "arguments.train=''\n",
    "arguments.dev=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if arguments.train:\n",
    "    train0 = load_sent(arguments.train + '.0', arguments.max_train_size)\n",
    "    train1 = load_sent(arguments.train + '.1', arguments.max_train_size)\n",
    "\n",
    "    print '#sents of training file 0:', len(train0)\n",
    "    print '#sents of training file 1:', len(train1)\n",
    "\n",
    "    if not os.path.isfile(arguments.vocab):\n",
    "        build_vocab(train0 + train1, arguments.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 9357\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary(arguments.vocab, arguments.embedding, arguments.dim_emb)\n",
    "print 'vocabulary size:', vocab.size\n",
    "\n",
    "if arguments.dev:\n",
    "    dev0 = load_sent(arguments.dev + '.0', arguments.max_train_size)\n",
    "    dev1 = load_sent(arguments.dev + '.1', arguments.max_train_size)\n",
    "\n",
    "if arguments.test:\n",
    "    test0 = load_sent(arguments.test + '.0', arguments.max_train_size)\n",
    "    test1 = load_sent(arguments.test + '.1', arguments.max_train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(): \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        model = create_model(sess, arguments, vocab)\n",
    "\n",
    "        if arguments.beam > 1:\n",
    "            decoder = beam_search.Decoder(sess, arguments, vocab, model)\n",
    "        else:\n",
    "            decoder = greedy_decoding.Decoder(sess, arguments, vocab, model)\n",
    "\n",
    "        if arguments.train:\n",
    "            batches, _, _ = get_batches(train0, train1, vocab.word2id,\n",
    "                arguments.batch_size, noisy=True)\n",
    "            random.shuffle(batches)\n",
    "\n",
    "            start_time = time.time()\n",
    "            step = 0\n",
    "            losses = Accumulator(arguments.steps_per_checkpoint,\n",
    "                ['loss', 'rec', 'adv', 'd0', 'd1'])\n",
    "            best_dev = float('inf')\n",
    "            learning_rate = arguments.learning_rate\n",
    "            rho = arguments.rho\n",
    "            gamma = arguments.gamma_init\n",
    "            dropout = arguments.dropout_keep_prob\n",
    "\n",
    "            #gradients = Accumulator(arguments.steps_per_checkpoint,\n",
    "            #    ['|grad_rec|', '|grad_adv|', '|grad|'])\n",
    "\n",
    "            for epoch in range(1, 1+arguments.max_epochs):\n",
    "                print '--------------------epoch %d--------------------' % epoch\n",
    "                print 'learning_rate:', learning_rate, '  gamma:', gamma, '  batches:', len(batches)\n",
    "\n",
    "                for batch in batches:\n",
    "#                     print 'Staring batch', len(batches)\n",
    "                    feed_dict = feed_dictionary(model, batch, rho, gamma,\n",
    "                        dropout, learning_rate)\n",
    "\n",
    "                    loss_d0, _ = sess.run([model.loss_d0, model.optimize_d0],\n",
    "                        feed_dict=feed_dict)\n",
    "                    loss_d1, _ = sess.run([model.loss_d1, model.optimize_d1],\n",
    "                        feed_dict=feed_dict)\n",
    "#                     print 'Session run completed'\n",
    "                    # do not back-propagate from the discriminator\n",
    "                    # when it is too poor\n",
    "                    if loss_d0 < 1.2 and loss_d1 < 1.2:\n",
    "                        optimize = model.optimize_tot\n",
    "                    else:\n",
    "                        optimize = model.optimize_rec\n",
    "#                     print 'Model optimized'\n",
    "                    loss, loss_rec, loss_adv, _ = sess.run([model.loss,\n",
    "                        model.loss_rec, model.loss_adv, optimize],\n",
    "                        feed_dict=feed_dict)\n",
    "                    losses.add([loss, loss_rec, loss_adv, loss_d0, loss_d1])\n",
    "#                     print('Losses calculated')\n",
    "                    #grad_rec, grad_adv, grad = sess.run([model.grad_rec_norm,\n",
    "                    #    model.grad_adv_norm, model.grad_norm],\n",
    "                    #    feed_dict=feed_dict)\n",
    "                    #gradients.add([grad_rec, grad_adv, grad])\n",
    "\n",
    "                    step += 1\n",
    "                    if step % arguments.steps_per_checkpoint == 0:\n",
    "                        losses.output('step %d, time %.0fs,'\n",
    "                            % (step, time.time() - start_time))\n",
    "                        losses.clear()\n",
    "\n",
    "                        #gradients.output()\n",
    "                        #gradients.clear()\n",
    "\n",
    "                if arguments.dev:\n",
    "                    dev_losses = transfer(model, decoder, sess, arguments, vocab,\n",
    "                        dev0, dev1, arguments.output + '.epoch%d' % epoch)\n",
    "                    dev_losses.output('dev')\n",
    "                    if dev_losses.values[0] < best_dev:\n",
    "                        best_dev = dev_losses.values[0]\n",
    "                        print 'saving model...'\n",
    "                        model.saver.save(sess, arguments.model)\n",
    "\n",
    "                gamma = max(arguments.gamma_min, gamma * arguments.gamma_decay)\n",
    "\n",
    "\n",
    "        if arguments.test:\n",
    "            test_losses = transfer(model, decoder, sess, arguments, vocab,\n",
    "                test0, test1, arguments.output)\n",
    "            test_losses.output('test')\n",
    "\n",
    "        if arguments.online_testing:\n",
    "            while True:\n",
    "                sys.stdout.write('> ')\n",
    "                sys.stdout.flush()\n",
    "                inp = sys.stdin.readline().rstrip()\n",
    "                if inp == 'quit' or inp == 'exit':\n",
    "                    break\n",
    "                inp = inp.split()\n",
    "                y = int(inp[0])\n",
    "                sent = inp[1:]\n",
    "\n",
    "                batch = get_batch([sent], [y], vocab.word2id)\n",
    "                ori, tsf = decoder.rewrite(batch)\n",
    "                print 'original:', ' '.join(w for w in ori[0])\n",
    "                print 'transfer:', ' '.join(w for w in tsf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../tmp/model\n",
      "INFO:tensorflow:Restoring parameters from ../tmp/model\n",
      "Output path:  ../tmp/sentiment.test\n",
      "Transfer in batches 24\n",
      "Write completed\n",
      "test loss 51.47, rec 49.73, adv 1.74, d0 1.18, d1 1.28\n"
     ]
    }
   ],
   "source": [
    "run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
